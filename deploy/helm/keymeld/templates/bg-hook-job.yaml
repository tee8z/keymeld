{{- if .Values.blueGreen.enabled }}
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ include "keymeld.fullname" . }}-bg-switchover
  labels:
    {{- include "keymeld.labels" . | nindent 4 }}
  annotations:
    argocd.argoproj.io/hook: PreSync
    argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
spec:
  backoffLimit: 0
  activeDeadlineSeconds: 600
  ttlSecondsAfterFinished: 300
  template:
    metadata:
      labels:
        {{- include "keymeld.selectorLabels" . | nindent 8 }}
        app.kubernetes.io/component: bg-switchover
    spec:
      serviceAccountName: {{ include "keymeld.fullname" . }}-bg-hook
      restartPolicy: Never
      containers:
        - name: switchover
          image: bitnami/kubectl:latest
          command: ["/bin/bash", "-c"]
          args:
            - |
              set -euo pipefail

              NAMESPACE="{{ .Release.Namespace }}"
              SERVICE="{{ include "keymeld.fullname" . }}"
              DEPLOY_PREFIX="{{ include "keymeld.fullname" . }}"
              CONTAINER="{{ .Chart.Name }}"
              NEW_IMAGE="{{ include "keymeld.image" . }}"
              MOUNT_PATH="{{ .Values.persistence.mountPath }}"

              echo "=== Blue/Green PreSync Switchover ==="
              echo "Namespace: $NAMESPACE"
              echo "Service: $SERVICE"
              echo "New image: $NEW_IMAGE"

              # Get current active slot from Service selector
              ACTIVE=$(kubectl -n "$NAMESPACE" get svc "$SERVICE" \
                -o jsonpath='{.spec.selector.app\.kubernetes\.io/slot}' 2>/dev/null || echo "")

              if [[ -z "$ACTIVE" ]]; then
                echo "No slot selector found on Service. First deploy - initializing."
                echo "Scaling up ${DEPLOY_PREFIX}-blue..."
                kubectl -n "$NAMESPACE" scale deployment "${DEPLOY_PREFIX}-blue" --replicas=1 || true
                echo "Waiting for ${DEPLOY_PREFIX}-blue to be ready..."
                kubectl -n "$NAMESPACE" rollout status deployment "${DEPLOY_PREFIX}-blue" --timeout=300s || true
                echo "Patching Service selector to blue..."
                kubectl -n "$NAMESPACE" patch svc "$SERVICE" --type=json \
                  -p '[{"op": "add", "path": "/spec/selector/app.kubernetes.io~1slot", "value": "blue"}]' || true
                echo "Scaling down ${DEPLOY_PREFIX}-green..."
                kubectl -n "$NAMESPACE" scale deployment "${DEPLOY_PREFIX}-green" --replicas=0 || true
                echo "First deploy initialization complete."
                exit 0
              fi

              if [[ "$ACTIVE" == "blue" ]]; then STANDBY="green"; else STANDBY="blue"; fi

              ACTIVE_DEPLOY="${DEPLOY_PREFIX}-${ACTIVE}"
              STANDBY_DEPLOY="${DEPLOY_PREFIX}-${STANDBY}"

              echo "Active: $ACTIVE ($ACTIVE_DEPLOY)"
              echo "Standby: $STANDBY ($STANDBY_DEPLOY)"

              CURRENT_IMAGE=$(kubectl -n "$NAMESPACE" get deployment "$ACTIVE_DEPLOY" \
                -o jsonpath="{.spec.template.spec.containers[?(@.name=='$CONTAINER')].image}" 2>/dev/null || echo "")

              echo "Current active image: $CURRENT_IMAGE"

              if [[ "$CURRENT_IMAGE" == "$NEW_IMAGE" ]]; then
                echo "Active deployment already running $NEW_IMAGE. No switchover needed."
                kubectl -n "$NAMESPACE" scale deployment "$STANDBY_DEPLOY" --replicas=0 2>/dev/null || true
                exit 0
              fi

              echo ""
              echo "=== Starting switchover: $ACTIVE -> $STANDBY ==="

              # Step 1: Update standby image
              echo "Step 1/11: Setting $STANDBY_DEPLOY image to $NEW_IMAGE"
              kubectl -n "$NAMESPACE" set image "deployment/$STANDBY_DEPLOY" "$CONTAINER=$NEW_IMAGE"

              # Step 2: Scale down active so Litestream flushes WAL to S3 on SIGTERM.
              # This must happen BEFORE the standby starts, otherwise the standby
              # restores stale data from S3 (the active's latest writes haven't
              # been synced yet).
              echo "Step 2/11: Scaling down $ACTIVE_DEPLOY (Litestream will flush WAL to S3)"
              kubectl -n "$NAMESPACE" scale deployment "$ACTIVE_DEPLOY" --replicas=0

              # Step 3: Wait for active pods to fully terminate so Litestream
              # completes its S3 sync on shutdown.
              echo "Step 3/11: Waiting for $ACTIVE_DEPLOY pods to terminate"
              kubectl -n "$NAMESPACE" wait --for=delete pod \
                -l "app.kubernetes.io/slot=$ACTIVE,app.kubernetes.io/name={{ include "keymeld.name" . }}" \
                --timeout=60s 2>/dev/null || true
              echo "$ACTIVE_DEPLOY terminated"

              # Step 4: Wait for Litestream S3 sync to propagate.
              # The pod is gone but the S3 PUT may still be in-flight or eventually-consistent.
              echo "Step 4/11: Waiting for Litestream S3 sync to propagate"
              sleep 10
              echo "S3 sync propagation window complete"

              # Step 5: Clean standby PVC so the litestream-restore init container
              # does a full restore from S3 instead of skipping (it uses -if-db-not-exists).
              # Without this, stale DB files on the standby PVC would be used as-is.
              echo "Step 5/11: Cleaning standby PVC (${DEPLOY_PREFIX}-${STANDBY})"
              CLEANUP_POD="pvc-cleanup-${STANDBY}-$(date +%s)"
              CLEANUP_CMD="echo Cleaning PVC files...; find $MOUNT_PATH -type f -name '*.db' -exec rm -fv {} +; find $MOUNT_PATH -type f -name '*.db-shm' -exec rm -fv {} +; find $MOUNT_PATH -type f -name '*.db-wal' -exec rm -fv {} +; find $MOUNT_PATH -type f -name '*.sqlite' -exec rm -fv {} +; find $MOUNT_PATH -type f -name '*.sqlite-shm' -exec rm -fv {} +; find $MOUNT_PATH -type f -name '*.sqlite-wal' -exec rm -fv {} +; find $MOUNT_PATH -type d -name '*-litestream' -exec rm -rfv {} + 2>/dev/null; echo PVC cleaned"
              printf '{"apiVersion":"v1","kind":"Pod","metadata":{"name":"%s"},"spec":{"restartPolicy":"Never","containers":[{"name":"cleanup","image":"alpine:latest","command":["sh","-c","%s"],"volumeMounts":[{"name":"data","mountPath":"%s"}]}],"volumes":[{"name":"data","persistentVolumeClaim":{"claimName":"%s"}}]}}' \
                "$CLEANUP_POD" "$CLEANUP_CMD" "$MOUNT_PATH" "${DEPLOY_PREFIX}-${STANDBY}" \
                | kubectl -n "$NAMESPACE" apply -f - 2>/dev/null || echo "WARNING: Failed to create cleanup pod"
              echo "Waiting for cleanup pod to complete..."
              for i in $(seq 1 60); do
                PHASE=$(kubectl -n "$NAMESPACE" get pod "$CLEANUP_POD" -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
                if [[ "$PHASE" == "Succeeded" || "$PHASE" == "Failed" ]]; then
                  echo "Cleanup pod finished with phase: $PHASE"
                  break
                fi
                sleep 1
              done
              kubectl -n "$NAMESPACE" logs "$CLEANUP_POD" 2>/dev/null || true
              kubectl -n "$NAMESPACE" delete pod "$CLEANUP_POD" --ignore-not-found 2>/dev/null || true

              # Step 6: Verify standby PVC is clean. If stale DB files remain,
              # the litestream-restore init container will skip restore (-if-db-not-exists)
              # and the standby starts with old/corrupt data.
              echo "Step 6/11: Verifying standby PVC is clean"
              VERIFY_POD="pvc-verify-${STANDBY}-$(date +%s)"
              printf '{"apiVersion":"v1","kind":"Pod","metadata":{"name":"%s"},"spec":{"restartPolicy":"Never","containers":[{"name":"verify","image":"alpine:latest","command":["sh","-c","find %s -name '"'"'*.db'"'"' -o -name '"'"'*.sqlite'"'"' -o -name '"'"'*-litestream'"'"' 2>/dev/null | wc -l"],"volumeMounts":[{"name":"data","mountPath":"%s"}]}],"volumes":[{"name":"data","persistentVolumeClaim":{"claimName":"%s"}}]}}' \
                "$VERIFY_POD" "$MOUNT_PATH" "$MOUNT_PATH" "${DEPLOY_PREFIX}-${STANDBY}" \
                | kubectl -n "$NAMESPACE" apply -f - 2>/dev/null || echo "WARNING: Failed to create verify pod"
              for i in $(seq 1 30); do
                PHASE=$(kubectl -n "$NAMESPACE" get pod "$VERIFY_POD" -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
                if [[ "$PHASE" == "Succeeded" || "$PHASE" == "Failed" ]]; then break; fi
                sleep 1
              done
              STALE_FILES=$(kubectl -n "$NAMESPACE" logs "$VERIFY_POD" 2>/dev/null | tr -d '[:space:]')
              kubectl -n "$NAMESPACE" delete pod "$VERIFY_POD" --ignore-not-found 2>/dev/null || true
              if [[ -n "$STALE_FILES" && "$STALE_FILES" -gt 0 ]] 2>/dev/null; then
                echo "ERROR: PVC cleanup failed: $STALE_FILES database files still present on ${DEPLOY_PREFIX}-${STANDBY}"
                echo "Rolling back: scaling up previous active"
                kubectl -n "$NAMESPACE" scale deployment "$ACTIVE_DEPLOY" --replicas=1
                exit 1
              fi
              echo "Standby PVC is clean, restore will fetch fresh data from S3"

              # Step 7: Scale up standby (triggers Litestream restore from fresh S3 data)
              echo "Step 7/11: Scaling up $STANDBY_DEPLOY (will restore DB from S3)"
              kubectl -n "$NAMESPACE" scale deployment "$STANDBY_DEPLOY" --replicas=1

              # Step 8: Wait for readiness
              echo "Step 8/11: Waiting for $STANDBY_DEPLOY to be ready..."
              if ! kubectl -n "$NAMESPACE" rollout status deployment "$STANDBY_DEPLOY" --timeout=300s; then
                echo "ERROR: Standby deployment failed readiness check"
                echo "Rolling back: scaling down standby, scaling up previous active"
                kubectl -n "$NAMESPACE" scale deployment "$STANDBY_DEPLOY" --replicas=0
                kubectl -n "$NAMESPACE" scale deployment "$ACTIVE_DEPLOY" --replicas=1
                exit 1
              fi

              # Step 9: Verify restored database integrity before routing traffic.
              # Uses a temporary pod pinned to the standby node for RWO PVC access.
              echo "Step 9/11: Verifying database integrity on standby"
              STANDBY_NODE=$(kubectl -n "$NAMESPACE" get pod \
                -l "app.kubernetes.io/slot=$STANDBY,app.kubernetes.io/name={{ include "keymeld.name" . }}" \
                -o jsonpath='{.items[0].spec.nodeName}' 2>/dev/null)
              INTEGRITY_POD="db-integrity-${STANDBY}-$(date +%s)"
              INTEGRITY_CMD="apk add --no-cache sqlite >/dev/null 2>&1 && find $MOUNT_PATH -name '*.db' -o -name '*.sqlite' | while read db; do result=\$(sqlite3 \"\$db\" 'PRAGMA quick_check;' 2>&1); if [ \"\$result\" != 'ok' ]; then echo \"FAILED:\$db:\$result\"; fi; done; echo DONE"
              printf '{"apiVersion":"v1","kind":"Pod","metadata":{"name":"%s"},"spec":{"nodeName":"%s","restartPolicy":"Never","containers":[{"name":"check","image":"alpine:latest","command":["sh","-c","%s"],"volumeMounts":[{"name":"data","mountPath":"%s"}]}],"volumes":[{"name":"data","persistentVolumeClaim":{"claimName":"%s"}}]}}' \
                "$INTEGRITY_POD" "$STANDBY_NODE" "$INTEGRITY_CMD" "$MOUNT_PATH" "${DEPLOY_PREFIX}-${STANDBY}" \
                | kubectl -n "$NAMESPACE" apply -f - 2>/dev/null || echo "WARNING: Failed to create integrity check pod"
              for i in $(seq 1 60); do
                PHASE=$(kubectl -n "$NAMESPACE" get pod "$INTEGRITY_POD" -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
                if [[ "$PHASE" == "Succeeded" || "$PHASE" == "Failed" ]]; then break; fi
                sleep 1
              done
              DB_INTEGRITY=$(kubectl -n "$NAMESPACE" logs "$INTEGRITY_POD" 2>/dev/null)
              kubectl -n "$NAMESPACE" delete pod "$INTEGRITY_POD" --ignore-not-found 2>/dev/null || true

              if echo "$DB_INTEGRITY" | grep -q "FAILED:"; then
                echo "WARNING: Integrity check failed, attempting REINDEX on affected databases..."

                # Scale down standby for exclusive PVC access during repair
                kubectl -n "$NAMESPACE" scale deployment "$STANDBY_DEPLOY" --replicas=0
                kubectl -n "$NAMESPACE" wait --for=delete pod \
                  -l "app.kubernetes.io/slot=$STANDBY,app.kubernetes.io/name={{ include "keymeld.name" . }}" \
                  --timeout=60s 2>/dev/null || true

                REPAIR_POD="db-repair-${STANDBY}-$(date +%s)"
                REPAIR_CMD="apk add --no-cache sqlite >/dev/null 2>&1 && find $MOUNT_PATH -name '*.db' -o -name '*.sqlite' | while read db; do sqlite3 \"\$db\" 'REINDEX; VACUUM;' 2>&1; result=\$(sqlite3 \"\$db\" 'PRAGMA integrity_check;' 2>&1); if [ \"\$result\" != 'ok' ]; then echo \"FAILED:\$db:\$result\"; fi; done; echo DONE"
                printf '{"apiVersion":"v1","kind":"Pod","metadata":{"name":"%s"},"spec":{"nodeName":"%s","restartPolicy":"Never","containers":[{"name":"repair","image":"alpine:latest","command":["sh","-c","%s"],"volumeMounts":[{"name":"data","mountPath":"%s"}]}],"volumes":[{"name":"data","persistentVolumeClaim":{"claimName":"%s"}}]}}' \
                  "$REPAIR_POD" "$STANDBY_NODE" "$REPAIR_CMD" "$MOUNT_PATH" "${DEPLOY_PREFIX}-${STANDBY}" \
                  | kubectl -n "$NAMESPACE" apply -f - 2>/dev/null || true
                for i in $(seq 1 60); do
                  PHASE=$(kubectl -n "$NAMESPACE" get pod "$REPAIR_POD" -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
                  if [[ "$PHASE" == "Succeeded" || "$PHASE" == "Failed" ]]; then break; fi
                  sleep 1
                done
                REPAIR_RESULT=$(kubectl -n "$NAMESPACE" logs "$REPAIR_POD" 2>/dev/null)
                kubectl -n "$NAMESPACE" delete pod "$REPAIR_POD" --ignore-not-found 2>/dev/null || true

                if echo "$REPAIR_RESULT" | grep -q "FAILED:"; then
                  echo "ERROR: Database integrity check failed after REINDEX"
                  echo "$REPAIR_RESULT" | grep "FAILED:"
                  echo "Rolling back: scaling up previous active"
                  kubectl -n "$NAMESPACE" scale deployment "$ACTIVE_DEPLOY" --replicas=1
                  exit 1
                fi
                echo "REINDEX repaired integrity issues, scaling standby back up"
                kubectl -n "$NAMESPACE" scale deployment "$STANDBY_DEPLOY" --replicas=1
                if ! kubectl -n "$NAMESPACE" rollout status deployment "$STANDBY_DEPLOY" --timeout=300s; then
                  echo "ERROR: Standby failed to restart after REINDEX, rolling back"
                  kubectl -n "$NAMESPACE" scale deployment "$STANDBY_DEPLOY" --replicas=0
                  kubectl -n "$NAMESPACE" scale deployment "$ACTIVE_DEPLOY" --replicas=1
                  exit 1
                fi
              else
                echo "Database integrity check passed"
              fi

              # Step 10: Flip Service selector (503 window ends)
              echo "Step 10/11: Flipping Service selector to $STANDBY"
              kubectl -n "$NAMESPACE" patch svc "$SERVICE" --type=json \
                -p "[{\"op\": \"replace\", \"path\": \"/spec/selector/app.kubernetes.io~1slot\", \"value\": \"$STANDBY\"}]"

              # Verify
              NEW_ACTIVE=$(kubectl -n "$NAMESPACE" get svc "$SERVICE" \
                -o jsonpath='{.spec.selector.app\.kubernetes\.io/slot}')
              echo "Service now pointing to: $NEW_ACTIVE"


              # Step 11: Scale down old active
              echo "Step 11/11: Scaling down old active $ACTIVE_DEPLOY"
              kubectl -n "$NAMESPACE" scale deployment "$ACTIVE_DEPLOY" --replicas=0 2>/dev/null || true
              echo ""
              echo "=== Switchover complete ==="
              echo "Active: $STANDBY ($STANDBY_DEPLOY) running $NEW_IMAGE"
              echo "Standby: $ACTIVE ($ACTIVE_DEPLOY) scaled to 0"
          resources:
            requests:
              memory: 32Mi
              cpu: 10m
            limits:
              memory: 64Mi
{{- end }}
